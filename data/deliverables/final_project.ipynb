{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter sentiment\n",
    "- this notebook was written by Austin Wilson and Paul Nguyen\n",
    "- in the first section we prepare the data for ingestion by our models\n",
    "    - we used two techniques for preprocessing ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Activation, Embedding, LSTM, SpatialDropout1D\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import f1_score,plot_precision_recall_curve, precision_score, recall_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/austinwilson/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/austinwilson/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/austinwilson/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "twitter_training.csv has the following attributes\n",
    "- Tweet_ID: unique identifier of the tweet (different for all tweets)\n",
    "- Entity: some topic that the tweet is related to\n",
    "- Sentiment: a view of or attitude toward a situation or event; an opinion\n",
    "- Text: text data entered as tweet by user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet_ID</th>\n",
       "      <th>Entity</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will murder yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Tweet_ID       Entity Sentiment  \\\n",
       "0      2401  Borderlands  Positive   \n",
       "1      2401  Borderlands  Positive   \n",
       "2      2401  Borderlands  Positive   \n",
       "3      2401  Borderlands  Positive   \n",
       "4      2401  Borderlands  Positive   \n",
       "\n",
       "                                                Text  \n",
       "0  im getting on borderlands and i will murder yo...  \n",
       "1  I am coming to the borders and I will kill you...  \n",
       "2  im getting on borderlands and i will kill you ...  \n",
       "3  im coming on borderlands and i will murder you...  \n",
       "4  im getting on borderlands 2 and i will murder ...  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('twitter_training.csv', names=[\"Tweet_ID\", \"Entity\", \"Sentiment\", \"Text\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text is the dependent variable we will use to predict Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will murder yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Positive</td>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Positive</td>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sentiment                                               Text\n",
       "0  Positive  im getting on borderlands and i will murder yo...\n",
       "1  Positive  I am coming to the borders and I will kill you...\n",
       "2  Positive  im getting on borderlands and i will kill you ...\n",
       "3  Positive  im coming on borderlands and i will murder you...\n",
       "4  Positive  im getting on borderlands 2 and i will murder ..."
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.drop(['Tweet_ID', 'Entity'], axis=1, inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Negative      22542\n",
       "Positive      20832\n",
       "Neutral       18318\n",
       "Irrelevant    12990\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we are only interested in positive and negative (binary classification)\n",
    "- remove neutral\n",
    "- remove irrelevant\n",
    "- make all text lower case\n",
    "- remove invaid text\n",
    "- remove rt (indicates retweet - not relevant to our purposes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data.Sentiment != \"Neutral\"]\n",
    "data = data[data.Sentiment != \"Irrelevant\"]\n",
    "data.Text = data.Text.apply(lambda x: str(x).lower())\n",
    "data.Text = data.Text.apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
    "data.Text = data.Text.replace('rt', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Negative    22542\n",
       "Positive    20832\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the data is balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## here we have two seperate preprocessing methods \n",
    "- one is using tfidf\n",
    "- the other is using nltk to tokenize and lemmatize the words\n",
    "- note that we will keep two datasets in memory to distinguish betwee the different approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_copy = data.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatiser = WordNetLemmatizer()\n",
    "stopwords = set(stopwords.words())\n",
    "def remove_stopwords(ls):\n",
    "    # Removes stop words and lemmatises\n",
    "    ls = [lemmatiser.lemmatize(word) for word in ls if word not in (stopwords) and (word.isalpha())]\n",
    "    \n",
    "    ls = \" \".join(ls)\n",
    "    return ls\n",
    "\n",
    "data.Text = data.Text.apply(word_tokenize)\n",
    "data.Text = data.Text.apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Positive</td>\n",
       "      <td>getting borderland murder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Positive</td>\n",
       "      <td>coming border kill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Positive</td>\n",
       "      <td>getting borderland kill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Positive</td>\n",
       "      <td>coming borderland murder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Positive</td>\n",
       "      <td>getting borderland murder</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sentiment                       Text\n",
       "0  Positive  getting borderland murder\n",
       "1  Positive         coming border kill\n",
       "2  Positive    getting borderland kill\n",
       "3  Positive   coming borderland murder\n",
       "4  Positive  getting borderland murder"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 1000\n",
    "tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
    "tokenizer.fit_on_texts(data.Text.values)\n",
    "X = tokenizer.texts_to_sequences(data.Text.values)\n",
    "X = pad_sequences(X)\n",
    "\n",
    "# save tokenizer for later \n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29060, 99) (29060, 2)\n",
      "(14314, 99) (14314, 2)\n"
     ]
    }
   ],
   "source": [
    "Y = pd.get_dummies(data.Sentiment).values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)\n",
    "print(X_train.shape,y_train.shape)\n",
    "print(X_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tfidf\n",
    "- create tfidf vectorizer\n",
    "- convert text to tfidf\n",
    "- one hot encode y\n",
    "- create training and testing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfvectorizer = TfidfVectorizer(stop_words='english', max_features=5000, min_df=5)\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = data_copy.Text\n",
    "tfidf_matrix=tfidfvectorizer.fit_transform(corpus)\n",
    "tfidf_data=tfidf_matrix.toarray()\n",
    "with open('vectorizer.pk', 'wb') as fin:\n",
    "    pickle.dump(tfidfvectorizer, fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2=tfidf_data\n",
    "y2= pd.get_dummies(data_copy.Sentiment).values\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2,y2, test_size = 0.33, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data for logistic regression\n",
    "y_lr=le.fit_transform(data.Sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lr, X_test_lr, y_train_lr, y_test_lr = train_test_split(X,y_lr, test_size = 0.33, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "verify the size of the training and test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29060, 5000) (29060, 2)\n",
      "(14314, 5000) (14314, 2)\n"
     ]
    }
   ],
   "source": [
    "print(X_train2.shape,y_train2.shape)\n",
    "print(X_test2.shape,y_test2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# models\n",
    "- the first is a LSTM (Long Short Term Memory)\n",
    "    - this is a recurrent nueral network which can improve accuracy of NLP problems\n",
    "    - able the detect context of text\n",
    "- Neural Network 1 \n",
    "    - this neural network is very minimal\n",
    "    - one dense layer with 25 notes and relu activation function\n",
    "    - the output is 2 nodes and softmax activation function\n",
    "- Neural Network 2\n",
    "    - here we have 4 layers with 25, 50,25,10 fully connected layers, respectively and all are relu activation function\n",
    "    - the output is 2 nodes and softmax activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_monitor = EarlyStopping(monitor='accuracy', patience=1)\n",
    "monitor=EarlyStopping(monitor='val_loss',min_delta=1e-3,patience=2,verbose=2,mode='auto')\n",
    "checkpointer1=ModelCheckpoint(filepath='best_weights1.hdf5',verbose=0,save_best_only=True)\n",
    "checkpointer2=ModelCheckpoint(filepath='best_weights2.hdf5',verbose=0,save_best_only=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 1000\n",
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "\n",
    "model = Sequential([\n",
    "     Embedding(max_features, embed_dim, input_length = X.shape[1]),\n",
    "     SpatialDropout1D(0.4),\n",
    "     LSTM(lstm_out, dropout=0.2),\n",
    "     Dense(2, activation='softmax')\n",
    "])\n",
    "checkpointer1=ModelCheckpoint(filepath='best_weights1.hdf5',verbose=0,save_best_only=True)\n",
    "monitor=EarlyStopping(monitor='val_loss',min_delta=1e-3,patience=2,verbose=2,mode='auto')\n",
    "model1=Sequential()\n",
    "model1.add(Dense(25, input_dim=X2.shape[1],activation='relu'))\n",
    "model1.add(Dense(2, activation='softmax'))\n",
    "# model 2 \n",
    "model2=Sequential()\n",
    "model2.add(Dense(25, input_dim=X2.shape[1],activation='relu'))\n",
    "model2.add(Dense(50, input_dim=X2.shape[1],activation='relu'))\n",
    "model2.add(Dense(25, input_dim=X2.shape[1],activation='relu'))\n",
    "model2.add(Dense(10, input_dim=X2.shape[1],activation='relu'))\n",
    "model2.add(Dense(2, activation='softmax'))\n",
    "# model 3 logistic regression\n",
    "model3 = LogisticRegression(verbose=1, solver='liblinear',random_state=42, C=5, penalty='l2', max_iter=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "909/909 [==============================] - 71s 78ms/step - loss: 0.4585 - accuracy: 0.7797\n",
      "Epoch 2/10\n",
      "909/909 [==============================] - 69s 76ms/step - loss: 0.3777 - accuracy: 0.8273\n",
      "Epoch 3/10\n",
      "909/909 [==============================] - 65s 71ms/step - loss: 0.3420 - accuracy: 0.8408\n",
      "Epoch 4/10\n",
      "909/909 [==============================] - 65s 71ms/step - loss: 0.3175 - accuracy: 0.8552\n",
      "Epoch 5/10\n",
      "909/909 [==============================] - 64s 71ms/step - loss: 0.3012 - accuracy: 0.8599\n",
      "Epoch 6/10\n",
      "909/909 [==============================] - 65s 72ms/step - loss: 0.2777 - accuracy: 0.8720\n",
      "Epoch 7/10\n",
      "909/909 [==============================] - 65s 71ms/step - loss: 0.2622 - accuracy: 0.8769\n",
      "Epoch 8/10\n",
      "909/909 [==============================] - 71s 78ms/step - loss: 0.2433 - accuracy: 0.8873\n",
      "Epoch 9/10\n",
      "909/909 [==============================] - 72s 79ms/step - loss: 0.2298 - accuracy: 0.8928\n",
      "Epoch 10/10\n",
      "909/909 [==============================] - 69s 76ms/step - loss: 0.2186 - accuracy: 0.8970\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fead7655e80>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "     loss='categorical_crossentropy',\n",
    "     optimizer='adam',\n",
    "     metrics=['accuracy']\n",
    ")\n",
    "model.fit(X_train, y_train, epochs = 10, batch_size=32, callbacks=[lstm_monitor], verbose = 1)\n",
    "model.save_weights(\"best_weights_lstm.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "909/909 - 1s - loss: 0.4227 - val_loss: 0.3201\n",
      "Epoch 2/100\n",
      "909/909 - 1s - loss: 0.2635 - val_loss: 0.2785\n",
      "Epoch 3/100\n",
      "909/909 - 1s - loss: 0.2219 - val_loss: 0.2642\n",
      "Epoch 4/100\n",
      "909/909 - 1s - loss: 0.1972 - val_loss: 0.2621\n",
      "Epoch 5/100\n",
      "909/909 - 1s - loss: 0.1795 - val_loss: 0.2584\n",
      "Epoch 6/100\n",
      "909/909 - 1s - loss: 0.1631 - val_loss: 0.2606\n",
      "Epoch 7/100\n",
      "909/909 - 1s - loss: 0.1495 - val_loss: 0.2607\n",
      "Epoch 00007: early stopping\n"
     ]
    }
   ],
   "source": [
    "model1.compile(loss='categorical_crossentropy',optimizer='adam')\n",
    "model1.fit(X_train2,y_train2,validation_data=(X_test2,y_test2),callbacks=[monitor,checkpointer1],verbose=2,epochs=100)\n",
    "model1.save_weights(\"best_weights_nn1.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "909/909 - 1s - loss: 0.4660 - val_loss: 0.3796\n",
      "Epoch 2/100\n",
      "909/909 - 1s - loss: 0.2843 - val_loss: 0.2971\n",
      "Epoch 3/100\n",
      "909/909 - 1s - loss: 0.2043 - val_loss: 0.2681\n",
      "Epoch 4/100\n",
      "909/909 - 1s - loss: 0.1648 - val_loss: 0.2679\n",
      "Epoch 5/100\n",
      "909/909 - 1s - loss: 0.1453 - val_loss: 0.2828\n",
      "Epoch 00005: early stopping\n"
     ]
    }
   ],
   "source": [
    "model2.compile(loss='categorical_crossentropy',optimizer='adam')\n",
    "model2.fit(X_train2,y_train2,validation_data=(X_test2,y_test2),callbacks=[monitor,checkpointer2],verbose=2,epochs=100)\n",
    "model2.save_weights(\"best_weights_nn2.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=5, max_iter=1000, random_state=42, solver='liblinear',\n",
       "                   verbose=1)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit(X_train_lr, y_train_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# score training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test)\n",
    "pred1 = model1.predict(X_test2)\n",
    "pred2 = model2.predict(X_test2)\n",
    "pred3 = model3.predict(X_test_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cm = confusion_matrix(np.argmax(y_test, axis=1), np.argmax(pred, axis=1))\n",
    "model1_cm = confusion_matrix(np.argmax(y_test2, axis=1), np.argmax(pred1, axis=1))\n",
    "model2_cm = confusion_matrix(np.argmax(y_test2, axis=1), np.argmax(pred2, axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cr = classification_report(np.argmax(y_test, axis=1), np.argmax(pred, axis=1))\n",
    "model1_cr = classification_report(np.argmax(y_test2, axis=1), np.argmax(pred1, axis=1))\n",
    "model2_cr = classification_report(np.argmax(y_test2, axis=1), np.argmax(pred2, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1226 6281]\n",
      " [ 826 5981]]\n",
      "[[6608  899]\n",
      " [ 592 6215]]\n",
      "[[6719  788]\n",
      " [ 590 6217]]\n"
     ]
    }
   ],
   "source": [
    "print(model_cm)\n",
    "print(model1_cm)\n",
    "print(model2_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.16      0.26      7507\n",
      "           1       0.49      0.88      0.63      6807\n",
      "\n",
      "    accuracy                           0.50     14314\n",
      "   macro avg       0.54      0.52      0.44     14314\n",
      "weighted avg       0.55      0.50      0.43     14314\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.88      0.90      7507\n",
      "           1       0.87      0.91      0.89      6807\n",
      "\n",
      "    accuracy                           0.90     14314\n",
      "   macro avg       0.90      0.90      0.90     14314\n",
      "weighted avg       0.90      0.90      0.90     14314\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.90      0.91      7507\n",
      "           1       0.89      0.91      0.90      6807\n",
      "\n",
      "    accuracy                           0.90     14314\n",
      "   macro avg       0.90      0.90      0.90     14314\n",
      "weighted avg       0.90      0.90      0.90     14314\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model_cr)\n",
    "print(model1_cr)\n",
    "print(model2_cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "38eed5c50e0a3327633343bf39af9b5ec442f97ac8dea3717efb1c3acd79de73"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
