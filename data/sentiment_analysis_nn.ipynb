{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Activation, Embedding, LSTM, SpatialDropout1D\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score,plot_precision_recall_curve, precision_score, recall_score\n",
    "\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intake the training data and only keep the necessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>im getting on borderlands and i will murder yo...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Sentiment\n",
       "0  im getting on borderlands and i will murder yo...  Positive\n",
       "1  I am coming to the borders and I will kill you...  Positive\n",
       "2  im getting on borderlands and i will kill you ...  Positive\n",
       "3  im coming on borderlands and i will murder you...  Positive\n",
       "4  im getting on borderlands 2 and i will murder ...  Positive"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('twitter_training.csv', names=[\"Tweet_ID\", \"Entity\", \"Sentiment\", \"Text\"])\n",
    "data = data[['Text','Sentiment']]\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Positive', 'Neutral', 'Negative', 'Irrelevant'], dtype=object)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Sentiment.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@GearboxOfficial @Borderlands I love(d) this game. But've spent 325hrs beating your game twice and farming like crazy to get which way those anoints that I want. And you say that that way that I'm playing wasn't which way it had intended so you weaken my favorite anoint...\""
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[425].Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yasss!!! Co-Stream with @jimmysgotya  twitch.tv/jimmysgotya'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[426].Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal is to identify Positive and Negative tweets, drop everything else and keep only valid text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data.Sentiment != \"Neutral\"]\n",
    "data = data[data.Sentiment != \"Irrelevant\"]\n",
    "data.Text = data.Text.apply(lambda x: str(x).lower())\n",
    "data.Text = data.Text.apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
    "\n",
    "# tfidf stuff \n",
    "corpus=data['Text']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Positive', 'Negative'], dtype=object)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Sentiment.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = ['negative', 'positive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfvectorizer= TfidfVectorizer(stop_words='english', max_features=5000, min_df=5)\n",
    "countvectorizer = CountVectorizer(analyzer='word', stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tfidf_matrix=tfidfvectorizer.fit_transform(corpus)\n",
    "tfidf_data=tfidf_matrix.toarray()\n",
    "\n",
    "countvectorizer_matrix = countvectorizer.fit_transform(corpus)\n",
    "count_data = countvectorizer_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43374, 5000)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43374, 22965)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tokens = countvectorizer.get_feature_names()\n",
    "tfidf_tokens = tfidfvectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29060, 5000) (29060, 2)\n",
      "(14314, 5000) (14314, 2)\n"
     ]
    }
   ],
   "source": [
    "x=tfidf_data\n",
    "y= pd.get_dummies(data.Sentiment).values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x,y, test_size = 0.33, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29060, 5000) (29060, 2)\n",
      "(14314, 5000) (14314, 2)\n"
     ]
    }
   ],
   "source": [
    "x2=count_data\n",
    "y2= pd.get_dummies(data.Sentiment).values\n",
    "X2_train, X2_test, Y2_train, Y2_test = train_test_split(x,y, test_size = 0.33, random_state = 42)\n",
    "print(X2_train.shape,Y2_train.shape)\n",
    "print(X2_test.shape,Y2_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer=ModelCheckpoint(filepath='best_weights1.hdf5',verbose=0,save_best_only=True)\n",
    "monitor=EarlyStopping(monitor='val_loss',min_delta=1e-3,patience=2,verbose=2,mode='auto')\n",
    "model1=Sequential()\n",
    "model1.add(Dense(25, input_dim=x.shape[1],activation='relu'))\n",
    "model1.add(Dense(2, activation='softmax'))\n",
    "model1.compile(loss='categorical_crossentropy',optimizer='adam')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 2 \n",
    "checkpointer=ModelCheckpoint(filepath='best_weights2.hdf5',verbose=0,save_best_only=True)\n",
    "monitor=EarlyStopping(monitor='val_loss',min_delta=1e-3,patience=2,verbose=2,mode='auto')\n",
    "model2=Sequential()\n",
    "model2.add(Dense(25, input_dim=x.shape[1],activation='relu'))\n",
    "model2.add(Dense(50, input_dim=x.shape[1],activation='relu'))\n",
    "model2.add(Dense(25, input_dim=x.shape[1],activation='relu'))\n",
    "model2.add(Dense(10, input_dim=x.shape[1],activation='relu'))\n",
    "\n",
    "model2.add(Dense(2, activation='softmax'))\n",
    "model2.compile(loss='categorical_crossentropy',optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 5000, 128)         128000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_2 (Spatial (None, 5000, 128)         0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 196)               254800    \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 2)                 394       \n",
      "=================================================================\n",
      "Total params: 383,194\n",
      "Trainable params: 383,194\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# model 3\n",
    "# model3\n",
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "max_features = 1000\n",
    "model3 = Sequential([\n",
    "     Embedding(max_features, embed_dim, input_length = x.shape[1]),\n",
    "     SpatialDropout1D(0.4),\n",
    "     LSTM(lstm_out, dropout=0.2),\n",
    "     Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "model3.compile(\n",
    "     loss='categorical_crossentropy',\n",
    "     optimizer='adam',\n",
    "     metrics=['accuracy']\n",
    ")\n",
    "print(model3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# monitor3=EarlyStopping(monitor='val_loss',min_delta=1e-3,patience=2,verbose=2,mode='auto')\n",
    "# checkpointer3=ModelCheckpoint(filepath='weights3.hdf5',verbose=0,save_best_only=True)\n",
    "# model3.fit(X_train, Y_train, epochs = 10, batch_size=32,validation_data=(X_test,Y_test),\n",
    "#     callbacks=[monitor3, checkpointer3], verbose = 1)\n",
    "# model3.save_weights(\"model3.hdf5\")\n",
    "model3.load_weights(\"weights.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "909/909 - 2s - loss: 0.3797 - val_loss: 0.2875\n",
      "Epoch 2/100\n",
      "909/909 - 1s - loss: 0.1946 - val_loss: 0.2150\n",
      "Epoch 3/100\n",
      "909/909 - 1s - loss: 0.1149 - val_loss: 0.2007\n",
      "Epoch 4/100\n",
      "909/909 - 1s - loss: 0.0816 - val_loss: 0.2219\n",
      "Epoch 5/100\n",
      "909/909 - 1s - loss: 0.0670 - val_loss: 0.2319\n",
      "Epoch 00005: early stopping\n"
     ]
    }
   ],
   "source": [
    "monitor2=EarlyStopping(monitor='val_loss',min_delta=1e-3,patience=2,verbose=2,mode='auto')\n",
    "checkpointer2=ModelCheckpoint(filepath='weights2.hdf5',verbose=0,save_best_only=True)\n",
    "model2.fit(X_train,Y_train,validation_data=(X_test,Y_test),callbacks=[monitor,checkpointer],verbose=2,epochs=100)\n",
    "model2.save_weights(\"model2.hdf5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "909/909 - 2s - loss: 0.4241 - val_loss: 0.3160\n",
      "Epoch 2/100\n",
      "909/909 - 1s - loss: 0.2639 - val_loss: 0.2801\n",
      "Epoch 3/100\n",
      "909/909 - 1s - loss: 0.2238 - val_loss: 0.2671\n",
      "Epoch 4/100\n",
      "909/909 - 1s - loss: 0.2005 - val_loss: 0.2616\n",
      "Epoch 5/100\n",
      "909/909 - 1s - loss: 0.1823 - val_loss: 0.2603\n",
      "Epoch 6/100\n",
      "909/909 - 1s - loss: 0.1678 - val_loss: 0.2625\n",
      "Epoch 7/100\n",
      "909/909 - 1s - loss: 0.1534 - val_loss: 0.2628\n",
      "Epoch 00007: early stopping\n"
     ]
    }
   ],
   "source": [
    "monitor1=EarlyStopping(monitor='val_loss',min_delta=1e-3,patience=2,verbose=2,mode='auto')\n",
    "checkpointer1=ModelCheckpoint(filepath='weights1.hdf5',verbose=0,save_best_only=True)\n",
    "\n",
    "model1.fit(X_train,Y_train,validation_data=(X_test,Y_test),callbacks=[monitor1,checkpointer1],verbose=2,epochs=100)\n",
    "model1.save_weights(\"model1.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = np.argmax(model1.predict(X_test), axis=1)\n",
    "#pred = np.argmax(pred, axis=1)\n",
    "pred2 = np.argmax(model2.predict(X_test), axis=1)\n",
    "#pred2 = np.argmax(pred2, axis=1)\n",
    "#pred3 = np.argmax(model3.predict(X_test), axis=1)\n",
    "\n",
    "# Y_test = np.argmax(Y_test, axis=1)\n",
    "Y_test = np.argmax(Y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1_score(Y_test, pred, average=\"weighted\")\n",
    "f1_model1 = f1_score(Y_test, pred1, average=\"weighted\")\n",
    "f1_model2 = f1_score(Y_test, pred2, average=\"weighted\")\n",
    "#f1_model3 = f1_score(Y_test, pred3, average=\"weighted\")\n",
    "\n",
    "recall_model1 = recall_score(Y_test, pred1)\n",
    "recall_model2 = recall_score(Y_test, pred2)\n",
    "#recall_model3 = recall_score(Y_test, pred3)\n",
    "\n",
    "\n",
    "precision_model1 = precision_score(Y_test, pred1)\n",
    "precision_model2 = precision_score(Y_test, pred2)\n",
    "#precision_model3 = precision_score(Y_test, pred3)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 scores for all models\n",
      "0.8952003335461977\n",
      "0.9218411008747404\n"
     ]
    }
   ],
   "source": [
    "print(\"f1 scores for all models\")\n",
    "print(f1_model1)\n",
    "print(f1_model2)\n",
    "# print(f1_model3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall scores\n",
      "0.915968855589834\n",
      "0.8946672542970472\n"
     ]
    }
   ],
   "source": [
    "print('recall scores')\n",
    "print(recall_model1)\n",
    "print(recall_model2)\n",
    "# print(recall_model3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision scores\n",
      "0.870323841429369\n",
      "0.938366718027735\n"
     ]
    }
   ],
   "source": [
    "print('precision scores')\n",
    "print(precision_model1)\n",
    "print(precision_model2)\n",
    "# print(precision_model3)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0e4631f8abf22b6e3bad75ef6fc74e8156bc6d7eda16a6da5468122f1c7f4a75"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
