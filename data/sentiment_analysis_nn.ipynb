{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Activation, Embedding, LSTM, SpatialDropout1D\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score,plot_precision_recall_curve, precision_score, recall_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet_ID</th>\n",
       "      <th>Entity</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will murder yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Tweet_ID       Entity Sentiment  \\\n",
       "0      2401  Borderlands  Positive   \n",
       "1      2401  Borderlands  Positive   \n",
       "2      2401  Borderlands  Positive   \n",
       "3      2401  Borderlands  Positive   \n",
       "4      2401  Borderlands  Positive   \n",
       "\n",
       "                                                Text  \n",
       "0  im getting on borderlands and i will murder yo...  \n",
       "1  I am coming to the borders and I will kill you...  \n",
       "2  im getting on borderlands and i will kill you ...  \n",
       "3  im coming on borderlands and i will murder you...  \n",
       "4  im getting on borderlands 2 and i will murder ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('twitter_training.csv', names=[\"Tweet_ID\", \"Entity\", \"Sentiment\", \"Text\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Borderlands', 'CallOfDutyBlackopsColdWar', 'Amazon', 'Overwatch',\n",
       "       'Xbox(Xseries)', 'NBA2K', 'Dota2', 'PlayStation5(PS5)',\n",
       "       'WorldOfCraft', 'CS-GO', 'Google', 'AssassinsCreed', 'ApexLegends',\n",
       "       'LeagueOfLegends', 'Fortnite', 'Microsoft', 'Hearthstone',\n",
       "       'Battlefield', 'PlayerUnknownsBattlegrounds(PUBG)', 'Verizon',\n",
       "       'HomeDepot', 'FIFA', 'RedDeadRedemption(RDR)', 'CallOfDuty',\n",
       "       'TomClancysRainbowSix', 'Facebook', 'GrandTheftAuto(GTA)',\n",
       "       'MaddenNFL', 'johnson&johnson', 'Cyberpunk2077',\n",
       "       'TomClancysGhostRecon', 'Nvidia'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Entity.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Positive', 'Neutral', 'Negative', 'Irrelevant'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Sentiment.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intake the training data and only keep the necessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>im getting on borderlands and i will murder yo...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Sentiment\n",
       "0  im getting on borderlands and i will murder yo...  Positive\n",
       "1  I am coming to the borders and I will kill you...  Positive\n",
       "2  im getting on borderlands and i will kill you ...  Positive\n",
       "3  im coming on borderlands and i will murder you...  Positive\n",
       "4  im getting on borderlands 2 and i will murder ...  Positive"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[['Text','Sentiment']]\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Positive', 'Neutral', 'Negative', 'Irrelevant'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Sentiment.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74682, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20832, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data.Sentiment == 'Positive'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22542, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data.Sentiment == 'Negative'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perfect casting for first Lilith, most anticipated film - Positive\n"
     ]
    }
   ],
   "source": [
    "print(f'{data.iloc[412].Text} - {data.iloc[412].Sentiment}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Positive'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[412].Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yasss!!! Co-Stream with @jimmysgotya  twitch.tv/jimmysgotya'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[426].Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal is to identify Positive and Negative tweets, drop everything else and keep only valid text\n",
    "- remove all rows with neutral or irrelevant sentiment\n",
    "- convert all text to lower case\n",
    "- remove emojis and special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data.Sentiment != \"Neutral\"]\n",
    "data = data[data.Sentiment != \"Irrelevant\"]\n",
    "data.Text = data.Text.apply(lambda x: str(x).lower())\n",
    "data.Text = data.Text.apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
    "\n",
    "# tfidf stuff \n",
    "corpus=data['Text']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Positive', 'Negative'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Sentiment.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = data.Sentiment.unique()[0]\n",
    "class_labels = ['negative', 'positive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfvectorizer= TfidfVectorizer(stop_words='english', max_features=5000, min_df=5)\n",
    "countvectorizer = CountVectorizer(analyzer='word', stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tfidf_matrix=tfidfvectorizer.fit_transform(corpus)\n",
    "tfidf_data=tfidf_matrix.toarray()\n",
    "\n",
    "countvectorizer_matrix = countvectorizer.fit_transform(corpus)\n",
    "count_data = countvectorizer_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43374, 5000)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43374, 22965)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tokens = countvectorizer.get_feature_names()\n",
    "tfidf_tokens = tfidfvectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Negative', 'Positive'], dtype='object')\n",
      "(29060, 5000) (29060, 2)\n",
      "(14314, 5000) (14314, 2)\n"
     ]
    }
   ],
   "source": [
    "x=tfidf_data\n",
    "columns= pd.get_dummies(data.Sentiment).columns\n",
    "print(columns)\n",
    "y= pd.get_dummies(data.Sentiment).values\n",
    "y2=le.fit_transform(data.Sentiment)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x,y, test_size = 0.33, random_state = 42)\n",
    "X_train, X_test, Y2_train, Y2_test = train_test_split(x,y2, test_size = 0.33, random_state = 42)\n",
    "\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 1, 1])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2\n",
    "Y2_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29060, 5000) (29060, 2)\n",
      "(14314, 5000) (14314, 2)\n"
     ]
    }
   ],
   "source": [
    "# this is for the count vectorizer (not used)\n",
    "\n",
    "\n",
    "# x2=count_data\n",
    "# y2= pd.get_dummies(data.Sentiment).values\n",
    "# X2_train, X2_test, Y2_train, Y2_test = train_test_split(x,y, test_size = 0.33, random_state = 42)\n",
    "# print(X2_train.shape,Y2_train.shape)\n",
    "# print(X2_test.shape,Y2_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer=ModelCheckpoint(filepath='best_weights1.hdf5',verbose=0,save_best_only=True)\n",
    "monitor=EarlyStopping(monitor='val_loss',min_delta=1e-3,patience=2,verbose=2,mode='auto')\n",
    "model1=Sequential()\n",
    "model1.add(Dense(25, input_dim=x.shape[1],activation='relu'))\n",
    "model1.add(Dense(2, activation='softmax'))\n",
    "model1.compile(loss='categorical_crossentropy',optimizer='adam')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 2 \n",
    "checkpointer=ModelCheckpoint(filepath='best_weights2.hdf5',verbose=0,save_best_only=True)\n",
    "monitor=EarlyStopping(monitor='val_loss',min_delta=1e-3,patience=2,verbose=2,mode='auto')\n",
    "model2=Sequential()\n",
    "model2.add(Dense(25, input_dim=x.shape[1],activation='relu'))\n",
    "model2.add(Dense(50, input_dim=x.shape[1],activation='relu'))\n",
    "model2.add(Dense(25, input_dim=x.shape[1],activation='relu'))\n",
    "model2.add(Dense(10, input_dim=x.shape[1],activation='relu'))\n",
    "\n",
    "model2.add(Dense(2, activation='softmax'))\n",
    "model2.compile(loss='categorical_crossentropy',optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 5000, 128)         128000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d (SpatialDr (None, 5000, 128)         0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 196)               254800    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 394       \n",
      "=================================================================\n",
      "Total params: 383,194\n",
      "Trainable params: 383,194\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# model 3\n",
    "# model3\n",
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "max_features = 1000\n",
    "model3 = Sequential([\n",
    "     Embedding(max_features, embed_dim, input_length = x.shape[1]),\n",
    "     SpatialDropout1D(0.4),\n",
    "     LSTM(lstm_out, dropout=0.2),\n",
    "     Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "model3.compile(\n",
    "     loss='categorical_crossentropy',\n",
    "     optimizer='adam',\n",
    "     metrics=['accuracy']\n",
    ")\n",
    "print(model3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 4 logistic regression\n",
    "model4 = LogisticRegression(verbose=1, solver='liblinear',random_state=42, C=5, penalty='l2', max_iter=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=5, max_iter=1000, random_state=42, solver='liblinear',\n",
       "                   verbose=1)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4.fit(X_train, Y2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "[[ 0.18410028  1.59505115 -0.31588795 ...  1.77707589  1.60727889\n",
      "  -2.85258206]]\n"
     ]
    }
   ],
   "source": [
    "print(model4.classes_)\n",
    "print(model4.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred4 = model4.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred4[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8723901299459775"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "f1_score(Y2_test, pred4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving model\n",
    "with open(\"model4.pkl\", 'wb') as file:\n",
    "    pickle.dump(model4, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model4.pkl\", 'rb') as file:\n",
    "    test_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "tes_pred = test_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8723901299459775"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(Y2_test, tes_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# monitor3=EarlyStopping(monitor='val_loss',min_delta=1e-3,patience=2,verbose=2,mode='auto')\n",
    "# checkpointer3=ModelCheckpoint(filepath='weights3.hdf5',verbose=0,save_best_only=True)\n",
    "# model3.fit(X_train, Y_train, epochs = 10, batch_size=32,validation_data=(X_test,Y_test),\n",
    "#     callbacks=[monitor3, checkpointer3], verbose = 1)\n",
    "# model3.save_weights(\"model3.hdf5\")\n",
    "model3.load_weights(\"weights.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "909/909 - 1s - loss: 0.0609 - val_loss: 0.2625\n",
      "Epoch 2/100\n",
      "909/909 - 1s - loss: 0.0564 - val_loss: 0.2942\n",
      "Epoch 3/100\n",
      "909/909 - 1s - loss: 0.0541 - val_loss: 0.3051\n",
      "Epoch 00003: early stopping\n"
     ]
    }
   ],
   "source": [
    "monitor2=EarlyStopping(monitor='val_loss',min_delta=1e-3,patience=2,verbose=2,mode='auto')\n",
    "checkpointer2=ModelCheckpoint(filepath='weights2.hdf5',verbose=0,save_best_only=True)\n",
    "model2.fit(X_train,Y_train,validation_data=(X_test,Y_test),callbacks=[monitor,checkpointer],verbose=2,epochs=100)\n",
    "model2.save_weights(\"model2.hdf5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "909/909 - 2s - loss: 0.4241 - val_loss: 0.3160\n",
      "Epoch 2/100\n",
      "909/909 - 1s - loss: 0.2639 - val_loss: 0.2801\n",
      "Epoch 3/100\n",
      "909/909 - 1s - loss: 0.2238 - val_loss: 0.2671\n",
      "Epoch 4/100\n",
      "909/909 - 1s - loss: 0.2005 - val_loss: 0.2616\n",
      "Epoch 5/100\n",
      "909/909 - 1s - loss: 0.1823 - val_loss: 0.2603\n",
      "Epoch 6/100\n",
      "909/909 - 1s - loss: 0.1678 - val_loss: 0.2625\n",
      "Epoch 7/100\n",
      "909/909 - 1s - loss: 0.1534 - val_loss: 0.2628\n",
      "Epoch 00007: early stopping\n"
     ]
    }
   ],
   "source": [
    "monitor1=EarlyStopping(monitor='val_loss',min_delta=1e-3,patience=2,verbose=2,mode='auto')\n",
    "checkpointer1=ModelCheckpoint(filepath='weights1.hdf5',verbose=0,save_best_only=True)\n",
    "\n",
    "model1.fit(X_train,Y_train,validation_data=(X_test,Y_test),callbacks=[monitor1,checkpointer1],verbose=2,epochs=100)\n",
    "model1.save_weights(\"model1.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = np.argmax(model1.predict(X_test), axis=1)\n",
    "#pred = np.argmax(pred, axis=1)\n",
    "pred2 = np.argmax(model2.predict(X_test), axis=1)\n",
    "#pred2 = np.argmax(pred2, axis=1)\n",
    "#pred3 = np.argmax(model3.predict(X_test), axis=1)\n",
    "\n",
    "# Y_test = np.argmax(Y_test, axis=1)\n",
    "Y_test = np.argmax(Y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1_score(Y_test, pred, average=\"weighted\")\n",
    "f1_model1 = f1_score(Y_test, pred1, average=\"weighted\")\n",
    "f1_model2 = f1_score(Y_test, pred2, average=\"weighted\")\n",
    "#f1_model3 = f1_score(Y_test, pred3, average=\"weighted\")\n",
    "\n",
    "recall_model1 = recall_score(Y_test, pred1)\n",
    "recall_model2 = recall_score(Y_test, pred2)\n",
    "#recall_model3 = recall_score(Y_test, pred3)\n",
    "\n",
    "\n",
    "precision_model1 = precision_score(Y_test, pred1)\n",
    "precision_model2 = precision_score(Y_test, pred2)\n",
    "#precision_model3 = precision_score(Y_test, pred3)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 scores for all models\n",
      "0.8952003335461977\n",
      "0.9218411008747404\n"
     ]
    }
   ],
   "source": [
    "print(\"f1 scores for all models\")\n",
    "print(f1_model1)\n",
    "print(f1_model2)\n",
    "# print(f1_model3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall scores\n",
      "0.915968855589834\n",
      "0.8946672542970472\n"
     ]
    }
   ],
   "source": [
    "print('recall scores')\n",
    "print(recall_model1)\n",
    "print(recall_model2)\n",
    "# print(recall_model3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision scores\n",
      "0.870323841429369\n",
      "0.938366718027735\n"
     ]
    }
   ],
   "source": [
    "print('precision scores')\n",
    "print(precision_model1)\n",
    "print(precision_model2)\n",
    "# print(precision_model3)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0e4631f8abf22b6e3bad75ef6fc74e8156bc6d7eda16a6da5468122f1c7f4a75"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
