{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intake the training data and only keep the necessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Twitter_training.csv', names=[\"Tweet_ID\", \"Entity\", \"Sentiment\", \"Text\"])\n",
    "data = data[['Text','Sentiment']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal is to identify Positive and Negative tweets, drop everything else and keep only valid text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "data = data[data.Sentiment != \"Neutral\"]\n",
    "data = data[data.Sentiment != \"Irrelevant\"]\n",
    "data.Text = data.Text.apply(lambda x: str(x).lower())\n",
    "data.Text = data.Text.apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatize words to elimate stopwords that provide no context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "stopwords = set(stopwords.words())\n",
    "def remove_stopwords(ls):\n",
    "    # Removes stop words and lemmatises\n",
    "    ls = [lemmatiser.lemmatize(word) for word in ls if word not in (stopwords) and (word.isalpha())]\n",
    "    \n",
    "    ls = \" \".join(ls)\n",
    "    return ls\n",
    "\n",
    "data.Text = data.Text.apply(word_tokenize)\n",
    "data.Text = data.Text.apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the words to eliminate variations of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[data.Sentiment == 'Positive'].size)\n",
    "print(data[data.Sentiment == 'Negative'].size)\n",
    "\n",
    "for idx,row in data.iterrows():\n",
    "    row[0] = row[0].replace('rt',' ')\n",
    "    \n",
    "max_features = 1000\n",
    "tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
    "tokenizer.fit_on_texts(data.Text.values)\n",
    "X = tokenizer.texts_to_sequences(data.Text.values)\n",
    "X = pad_sequences(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of data after applying transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we used LSTM, a recurrent neural network implmentation, to differentiate and distinguish the context of the content as the method to determining sentiment. \n",
    "\n",
    "The LSTM layer only uses the dropout and not the recurrent_dropout parameter in order to accelerate training. Recurrent_dropout is currently not supported by Nvidia CUDNN and will prevent the model from utilizing GPU acceleration.\n",
    "\n",
    "The Dense layer should only be 2 units as our sentiment has only 2 possible values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "\n",
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "\n",
    "model = Sequential([\n",
    "     Embedding(max_features, embed_dim, input_length = X.shape[1]),\n",
    "     SpatialDropout1D(0.4),\n",
    "     LSTM(lstm_out, dropout=0.2),\n",
    "     Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "     loss='categorical_crossentropy',\n",
    "     optimizer='adam',\n",
    "     metrics=['accuracy']\n",
    ")\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = pd.get_dummies(data.Sentiment).values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "callback = EarlyStopping(monitor='accuracy', patience=4)\n",
    "\n",
    "model.fit(X_train, Y_train, epochs = 10, batch_size=32, callbacks=[callback], verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_size = 1500\n",
    "\n",
    "X_validate = X_test[-validation_size:]\n",
    "Y_validate = Y_test[-validation_size:]\n",
    "X_test = X_test[:-validation_size]\n",
    "Y_test = Y_test[:-validation_size]\n",
    "score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = 32)\n",
    "print(\"score: %.2f\" % (score))\n",
    "print(\"acc: %.2f\" % (acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run tests on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = {\n",
    "    'pos_cnt':0, \n",
    "    'neg_cnt':0, \n",
    "    'pos_correct':0, \n",
    "    'neg_correct':0\n",
    "}\n",
    "\n",
    "def inc(count):\n",
    "    count+=1\n",
    "\n",
    "for x in range(len(X_validate)):\n",
    "    result = model.predict(X_validate[x].reshape(1,X_test.shape[1]), batch_size=1)[0]\n",
    "   \n",
    "    if np.argmax(result) == np.argmax(Y_validate[x]):\n",
    "        accuracy['neg_correct' if np.argmax(Y_validate[x]) == 0 else 'pos_correct'] +=1\n",
    "       \n",
    "    accuracy['neg_cnt' if np.argmax(Y_validate[x]) == 0 else 'pos_cnt'] +=1\n",
    "\n",
    "print(\"pos_acc\", accuracy['pos_correct']/accuracy['pos_cnt']*100, \"%\")\n",
    "print(\"neg_acc\", accuracy['neg_correct']/accuracy['neg_cnt']*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize the tweet by the pre-fitted tokenizer instance then pad the tweet to have the same dimensions as the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_prediction(twt):\n",
    "    twtData = tokenizer.texts_to_sequences([twt])\n",
    "    twtData = pad_sequences(twtData, maxlen=28, dtype='int32', value=0)\n",
    "    print(twtData)\n",
    "    sentiment = model.predict(twtData,batch_size=1,verbose = 2)[0]\n",
    "    sentimentValue = \"negative\" if(np.argmax(sentiment) == 0) else \"positive\"\n",
    "    return sentimentValue\n",
    "    \n",
    "twt = 'The new CoD is pretty lit'\n",
    "print(apply_prediction(twt))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0e4631f8abf22b6e3bad75ef6fc74e8156bc6d7eda16a6da5468122f1c7f4a75"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
